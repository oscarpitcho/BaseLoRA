{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25bac80c",
      "metadata": {},
      "source": [
        "# Adapter Demo: Training and Batched Inference with Geneformer\n",
        "\n",
        "This notebook demonstrates the multi-adapter LoRA workflow on Geneformer v2:\n",
        "1. **Training**: Register fresh adapters, train them on random data, save to disk\n",
        "2. **Inference**: Load adapters from disk and run batched inference with per-sample adapter selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "efbf500a",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-22 19:22:43,486 - INFO:datasets:PyTorch version 2.7.0+cpu available.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x793d2409d710>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"src\")\n",
        "\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from helical.models.geneformer.model import Geneformer, GeneformerConfig\n",
        "\n",
        "from adapter_manager import AdapterManager\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = \"gf-12L-38M-i4096\"  # Medium Geneformer v2: 38M params, 12 layers\n",
        "VOCAB_SIZE = 20275  # Geneformer's gene vocabulary size\n",
        "SEQ_LEN = 512  # Shorter sequence for demo (max is 4096)\n",
        "R = 8\n",
        "ALPHA = 8\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "ADAPTERS_DIR = Path(\"./adapters\")\n",
        "ADAPTERS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cb0d9ccb",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-22 19:22:47,111 - INFO:helical.models.geneformer.model:Model finished initializing.\n",
            "2026-01-22 19:22:47,112 - INFO:helical.models.geneformer.model:'gf-12L-38M-i4096' model is in 'eval' mode, on device 'cpu' with embedding mode 'cell'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: gf-12L-38M-i4096\n",
            "Model type: <class 'transformers.models.bert.modeling_bert.BertModel'>\n",
            "LoRA layers injected: 72\n",
            "Sample layers: ['encoder.layer.0.attention.self.query', 'encoder.layer.0.attention.self.key', 'encoder.layer.0.attention.self.value', 'encoder.layer.0.attention.output.dense', 'encoder.layer.0.intermediate.dense']\n"
          ]
        }
      ],
      "source": [
        "# Load Geneformer model via helical\n",
        "config = GeneformerConfig(model_name=MODEL_NAME, batch_size=BATCH_SIZE)\n",
        "geneformer = Geneformer(config)\n",
        "\n",
        "# Access the underlying HuggingFace BertForMaskedLM model\n",
        "base_model = geneformer.model.bert  # BertModel, not BertForMaskedLM\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Model type: {type(base_model)}\")\n",
        "\n",
        "# Wrap with AdapterManager - injects BatchedLoRALinear into ALL Linear layers\n",
        "manager = AdapterManager(base_model, r=R, alpha=ALPHA, max_cache_entries=0)\n",
        "\n",
        "print(f\"LoRA layers injected: {len(manager.lora_names)}\")\n",
        "print(f\"Sample layers: {manager.lora_names[:5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c90dae0",
      "metadata": {},
      "source": [
        "## Part 1: Training Adapters - Batched\n",
        "\n",
        "Register 3 fresh adapters and train on random token data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "39a6cc53",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Registered: adapter_1\n",
            "Registered: adapter_2\n",
            "Registered: adapter_3\n",
            "Total adapters: ['adapter_1', 'adapter_2', 'adapter_3']\n"
          ]
        }
      ],
      "source": [
        "# Register 3 fresh adapters (Kaiming init for A, zeros for B)\n",
        "adapter_names = [\"adapter_1\", \"adapter_2\", \"adapter_3\"]\n",
        "\n",
        "for name in adapter_names:\n",
        "    manager.register_new_adapter(name)\n",
        "    print(f\"Registered: {name}\")\n",
        "\n",
        "print(f\"Total adapters: {list(manager.registered_adapters.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3fb1e38a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch   0 | Loss: 2.1993\n",
            "Epoch   1 | Loss: 2.2446\n",
            "Epoch   2 | Loss: 2.2034\n",
            "Epoch   3 | Loss: 2.1801\n",
            "Epoch   4 | Loss: 2.1705\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 5\n",
        "NUM_BATCHES = 3\n",
        "\n",
        "manager.set_adapters(adapter_names)\n",
        "\n",
        "# Optimizer only trains LoRA parameters (base model frozen)\n",
        "optimizer = torch.optim.Adam(base_model.parameters(), lr=1e-4)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "with manager.training_mode(adapter_names):\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        epoch_loss = 0.0\n",
        "        \n",
        "        for _ in range(NUM_BATCHES):\n",
        "            # Random token IDs (simulating rank-value encoded genes)\n",
        "            # Token 0 is padding, tokens 1-25425 are genes\n",
        "            x = torch.randint(1, VOCAB_SIZE, (BATCH_SIZE, SEQ_LEN))\n",
        "            \n",
        "            # Per-sample adapter assignment\n",
        "            adapter_ids = [adapter_names[i % len(adapter_names)] for i in range(BATCH_SIZE)]\n",
        "            \n",
        "            # Random targets (simulating embedding regression task)\n",
        "            # Geneformer hidden dim = 512\n",
        "            targets = torch.randn(BATCH_SIZE, 512)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            outputs = manager.forward_multi(x, adapter_ids)\n",
        "            \n",
        "            # Use CLS token embedding (position 0)\n",
        "            embeddings = outputs[:, 0, :]\n",
        "            \n",
        "            loss = loss_fn(embeddings, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "        print(f\"Epoch {epoch:3d} | Loss: {epoch_loss / NUM_BATCHES:.4f}\")\n",
        "\n",
        "print(\"Training complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2003aa3c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: adapters/adapter_1.safetensors\n",
            "Saved: adapters/adapter_2.safetensors\n",
            "Saved: adapters/adapter_3.safetensors\n",
            "Adapter files: [PosixPath('adapters/adapter_2.safetensors'), PosixPath('adapters/adapter_3.safetensors'), PosixPath('adapters/adapter_1.safetensors')]\n"
          ]
        }
      ],
      "source": [
        "# Save trained adapters to disk\n",
        "for name in adapter_names:\n",
        "    path = ADAPTERS_DIR / f\"{name}.safetensors\"\n",
        "    manager.save_adapter(name, str(path))\n",
        "    print(f\"Saved: {path}\")\n",
        "\n",
        "print(f\"Adapter files: {list(ADAPTERS_DIR.glob('*.safetensors'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c6cfcd2",
      "metadata": {},
      "source": [
        "## Part 2: Batched Inference with Multiple Adapters\n",
        "\n",
        "Create fresh model, load saved adapters, run batched inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c2d5093d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-22 19:23:10,698 - INFO:helical.models.geneformer.model:Model finished initializing.\n",
            "2026-01-22 19:23:10,699 - INFO:helical.models.geneformer.model:'gf-12L-38M-i4096' model is in 'eval' mode, on device 'cpu' with embedding mode 'cell'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformers.models.bert.modeling_bert.BertModel'>\n",
            "Loaded: adapter_1\n",
            "Loaded: adapter_2\n",
            "Loaded: adapter_3\n",
            "Registered adapters: ['adapter_1', 'adapter_2', 'adapter_3']\n"
          ]
        }
      ],
      "source": [
        "# Fresh model (simulating production deployment)\n",
        "inference_config = GeneformerConfig(model_name=MODEL_NAME, batch_size=BATCH_SIZE)\n",
        "inference_geneformer = Geneformer(inference_config)\n",
        "inference_model = inference_geneformer.model.bert  # same as training\n",
        "\n",
        "print(type(inference_model))\n",
        "inference_manager = AdapterManager(\n",
        "    inference_model, r=R, alpha=ALPHA, max_cache_entries=100\n",
        ")\n",
        "\n",
        "# Load trained adapters\n",
        "for name in adapter_names:\n",
        "    path = ADAPTERS_DIR / f\"{name}.safetensors\"\n",
        "    inference_manager.register_adapter(name, str(path))\n",
        "    print(f\"Loaded: {name}\")\n",
        "\n",
        "print(f\"Registered adapters: {list(inference_manager.registered_adapters.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e19c0f23",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([6, 512])\n",
            "Adapter assignments: ['adapter_1', 'adapter_2', 'adapter_3', 'adapter_1', 'adapter_2', 'adapter_3']\n",
            "Output shape: torch.Size([6, 512, 512])\n",
            "Embedding shape: torch.Size([6, 512])\n"
          ]
        }
      ],
      "source": [
        "batch_size = 6\n",
        "x = torch.randint(1, VOCAB_SIZE, (batch_size, SEQ_LEN))\n",
        "\n",
        "# Each sample uses different adapter - processed in single forward pass\n",
        "adapter_ids = [\"adapter_1\", \"adapter_2\", \"adapter_3\", \"adapter_1\", \"adapter_2\", \"adapter_3\"]\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Adapter assignments: {adapter_ids}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = inference_manager.forward_multi(x, adapter_ids)\n",
        "    embeddings = outputs[:, 0, :]  # CLS token embeddings\n",
        "\n",
        "print(f\"Output shape: {outputs.shape}\")\n",
        "print(f\"Embedding shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b86b21eb",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated multi-adapter LoRA on **Geneformer v2 (gf-12L-38M-i4096)**:\n",
        "\n",
        "1. **Model**: 38M parameter transformer with 12 encoder layers\n",
        "2. **LoRA injection**: All Linear layers wrapped with BatchedLoRALinear\n",
        "3. **Training**: Multiple adapters trained simultaneously with per-sample selection\n",
        "4. **Inference**: Batched forward pass with mixed adapter assignments (zero switching overhead)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b512881",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
