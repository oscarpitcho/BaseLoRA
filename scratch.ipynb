{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e5cd5138",
      "metadata": {},
      "outputs": [],
      "source": [
        "import helical"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0f6b29a7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-21 19:27:17,574 - INFO:datasets:PyTorch version 2.7.0+cpu available.\n",
            "2026-01-21 19:27:22,726 - INFO:helical.models.geneformer.model:Model finished initializing.\n",
            "2026-01-21 19:27:22,727 - INFO:helical.models.geneformer.model:'gf-6L-10M-i2048' model is in 'eval' mode, on device 'cpu' with embedding mode 'cell'.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model architecture:\n",
            "BertForMaskedLM(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(25426, 256, padding_idx=0)\n",
            "      (position_embeddings): Embedding(2048, 256)\n",
            "      (token_type_embeddings): Embedding(2, 256)\n",
            "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.02, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-5): 6 x BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSdpaSelfAttention(\n",
            "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (dropout): Dropout(p=0.02, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.02, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
            "            (intermediate_act_fn): ReLU()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
            "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.02, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (cls): BertOnlyMLMHead(\n",
            "    (predictions): BertLMPredictionHead(\n",
            "      (transform): BertPredictionHeadTransform(\n",
            "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
            "        (transform_act_fn): ReLU()\n",
            "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "      (decoder): Linear(in_features=256, out_features=25426, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "\n",
            "Named modules (Linear layers only):\n",
            "\n",
            "bert\n",
            "bert.embeddings\n",
            "bert.embeddings.word_embeddings\n",
            "bert.embeddings.position_embeddings\n",
            "bert.embeddings.token_type_embeddings\n",
            "bert.embeddings.LayerNorm\n",
            "bert.embeddings.dropout\n",
            "bert.encoder\n",
            "bert.encoder.layer\n",
            "bert.encoder.layer.0\n",
            "bert.encoder.layer.0.attention\n",
            "bert.encoder.layer.0.attention.self\n",
            "bert.encoder.layer.0.attention.self.query\n",
            "bert.encoder.layer.0.attention.self.key\n",
            "bert.encoder.layer.0.attention.self.value\n",
            "bert.encoder.layer.0.attention.self.dropout\n",
            "bert.encoder.layer.0.attention.output\n",
            "bert.encoder.layer.0.attention.output.dense\n",
            "bert.encoder.layer.0.attention.output.LayerNorm\n",
            "bert.encoder.layer.0.attention.output.dropout\n",
            "bert.encoder.layer.0.intermediate\n",
            "bert.encoder.layer.0.intermediate.dense\n",
            "bert.encoder.layer.0.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.0.output\n",
            "bert.encoder.layer.0.output.dense\n",
            "bert.encoder.layer.0.output.LayerNorm\n",
            "bert.encoder.layer.0.output.dropout\n",
            "bert.encoder.layer.1\n",
            "bert.encoder.layer.1.attention\n",
            "bert.encoder.layer.1.attention.self\n",
            "bert.encoder.layer.1.attention.self.query\n",
            "bert.encoder.layer.1.attention.self.key\n",
            "bert.encoder.layer.1.attention.self.value\n",
            "bert.encoder.layer.1.attention.self.dropout\n",
            "bert.encoder.layer.1.attention.output\n",
            "bert.encoder.layer.1.attention.output.dense\n",
            "bert.encoder.layer.1.attention.output.LayerNorm\n",
            "bert.encoder.layer.1.attention.output.dropout\n",
            "bert.encoder.layer.1.intermediate\n",
            "bert.encoder.layer.1.intermediate.dense\n",
            "bert.encoder.layer.1.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.1.output\n",
            "bert.encoder.layer.1.output.dense\n",
            "bert.encoder.layer.1.output.LayerNorm\n",
            "bert.encoder.layer.1.output.dropout\n",
            "bert.encoder.layer.2\n",
            "bert.encoder.layer.2.attention\n",
            "bert.encoder.layer.2.attention.self\n",
            "bert.encoder.layer.2.attention.self.query\n",
            "bert.encoder.layer.2.attention.self.key\n",
            "bert.encoder.layer.2.attention.self.value\n",
            "bert.encoder.layer.2.attention.self.dropout\n",
            "bert.encoder.layer.2.attention.output\n",
            "bert.encoder.layer.2.attention.output.dense\n",
            "bert.encoder.layer.2.attention.output.LayerNorm\n",
            "bert.encoder.layer.2.attention.output.dropout\n",
            "bert.encoder.layer.2.intermediate\n",
            "bert.encoder.layer.2.intermediate.dense\n",
            "bert.encoder.layer.2.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.2.output\n",
            "bert.encoder.layer.2.output.dense\n",
            "bert.encoder.layer.2.output.LayerNorm\n",
            "bert.encoder.layer.2.output.dropout\n",
            "bert.encoder.layer.3\n",
            "bert.encoder.layer.3.attention\n",
            "bert.encoder.layer.3.attention.self\n",
            "bert.encoder.layer.3.attention.self.query\n",
            "bert.encoder.layer.3.attention.self.key\n",
            "bert.encoder.layer.3.attention.self.value\n",
            "bert.encoder.layer.3.attention.self.dropout\n",
            "bert.encoder.layer.3.attention.output\n",
            "bert.encoder.layer.3.attention.output.dense\n",
            "bert.encoder.layer.3.attention.output.LayerNorm\n",
            "bert.encoder.layer.3.attention.output.dropout\n",
            "bert.encoder.layer.3.intermediate\n",
            "bert.encoder.layer.3.intermediate.dense\n",
            "bert.encoder.layer.3.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.3.output\n",
            "bert.encoder.layer.3.output.dense\n",
            "bert.encoder.layer.3.output.LayerNorm\n",
            "bert.encoder.layer.3.output.dropout\n",
            "bert.encoder.layer.4\n",
            "bert.encoder.layer.4.attention\n",
            "bert.encoder.layer.4.attention.self\n",
            "bert.encoder.layer.4.attention.self.query\n",
            "bert.encoder.layer.4.attention.self.key\n",
            "bert.encoder.layer.4.attention.self.value\n",
            "bert.encoder.layer.4.attention.self.dropout\n",
            "bert.encoder.layer.4.attention.output\n",
            "bert.encoder.layer.4.attention.output.dense\n",
            "bert.encoder.layer.4.attention.output.LayerNorm\n",
            "bert.encoder.layer.4.attention.output.dropout\n",
            "bert.encoder.layer.4.intermediate\n",
            "bert.encoder.layer.4.intermediate.dense\n",
            "bert.encoder.layer.4.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.4.output\n",
            "bert.encoder.layer.4.output.dense\n",
            "bert.encoder.layer.4.output.LayerNorm\n",
            "bert.encoder.layer.4.output.dropout\n",
            "bert.encoder.layer.5\n",
            "bert.encoder.layer.5.attention\n",
            "bert.encoder.layer.5.attention.self\n",
            "bert.encoder.layer.5.attention.self.query\n",
            "bert.encoder.layer.5.attention.self.key\n",
            "bert.encoder.layer.5.attention.self.value\n",
            "bert.encoder.layer.5.attention.self.dropout\n",
            "bert.encoder.layer.5.attention.output\n",
            "bert.encoder.layer.5.attention.output.dense\n",
            "bert.encoder.layer.5.attention.output.LayerNorm\n",
            "bert.encoder.layer.5.attention.output.dropout\n",
            "bert.encoder.layer.5.intermediate\n",
            "bert.encoder.layer.5.intermediate.dense\n",
            "bert.encoder.layer.5.intermediate.intermediate_act_fn\n",
            "bert.encoder.layer.5.output\n",
            "bert.encoder.layer.5.output.dense\n",
            "bert.encoder.layer.5.output.LayerNorm\n",
            "bert.encoder.layer.5.output.dropout\n",
            "cls\n",
            "cls.predictions\n",
            "cls.predictions.transform\n",
            "cls.predictions.transform.dense\n",
            "cls.predictions.transform.transform_act_fn\n",
            "cls.predictions.transform.LayerNorm\n",
            "cls.predictions.decoder\n"
          ]
        }
      ],
      "source": [
        "from helical.models.geneformer import Geneformer, GeneformerConfig\n",
        "import torch\n",
        "\n",
        "# Create config for a smaller model\n",
        "config = GeneformerConfig(model_name='gf-6L-10M-i2048', batch_size=1)\n",
        "model = Geneformer(config)\n",
        "\n",
        "# Print the model architecture\n",
        "print('Model architecture:')\n",
        "print(model.model)\n",
        "print()\n",
        "\n",
        "# List all named modules with their types\n",
        "print('Named modules (Linear layers only):')\n",
        "for name, module in model.model.named_modules():\n",
        "    print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4799f0af",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def find_linear_layers(model: nn.Module) -> dict[str, nn.Linear]:\n",
        "    \"\"\"Find all Linear layers in a model with their full path names.\"\"\"\n",
        "    linear_layers = {}\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Linear):\n",
        "            linear_layers[name] = module\n",
        "    return linear_layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "1e1efed0",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'bert.encoder.layer.0.attention.self.query': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.0.attention.self.key': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.0.attention.self.value': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.0.attention.output.dense': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.0.intermediate.dense': Linear(in_features=256, out_features=512, bias=True),\n",
              " 'bert.encoder.layer.0.output.dense': Linear(in_features=512, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.1.attention.self.query': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.1.attention.self.key': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.1.attention.self.value': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.1.attention.output.dense': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.1.intermediate.dense': Linear(in_features=256, out_features=512, bias=True),\n",
              " 'bert.encoder.layer.1.output.dense': Linear(in_features=512, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.2.attention.self.query': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.2.attention.self.key': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.2.attention.self.value': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.2.attention.output.dense': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.2.intermediate.dense': Linear(in_features=256, out_features=512, bias=True),\n",
              " 'bert.encoder.layer.2.output.dense': Linear(in_features=512, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.3.attention.self.query': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.3.attention.self.key': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.3.attention.self.value': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.3.attention.output.dense': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.3.intermediate.dense': Linear(in_features=256, out_features=512, bias=True),\n",
              " 'bert.encoder.layer.3.output.dense': Linear(in_features=512, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.4.attention.self.query': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.4.attention.self.key': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.4.attention.self.value': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.4.attention.output.dense': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.4.intermediate.dense': Linear(in_features=256, out_features=512, bias=True),\n",
              " 'bert.encoder.layer.4.output.dense': Linear(in_features=512, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.5.attention.self.query': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.5.attention.self.key': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.5.attention.self.value': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.5.attention.output.dense': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'bert.encoder.layer.5.intermediate.dense': Linear(in_features=256, out_features=512, bias=True),\n",
              " 'bert.encoder.layer.5.output.dense': Linear(in_features=512, out_features=256, bias=True),\n",
              " 'cls.predictions.transform.dense': Linear(in_features=256, out_features=256, bias=True),\n",
              " 'cls.predictions.decoder': Linear(in_features=256, out_features=25426, bias=True)}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "find_linear_layers(model.model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "29ce5850",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('',\n",
              "  BertForMaskedLM(\n",
              "    (bert): BertModel(\n",
              "      (embeddings): BertEmbeddings(\n",
              "        (word_embeddings): Embedding(25426, 256, padding_idx=0)\n",
              "        (position_embeddings): Embedding(2048, 256)\n",
              "        (token_type_embeddings): Embedding(2, 256)\n",
              "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "      (encoder): BertEncoder(\n",
              "        (layer): ModuleList(\n",
              "          (0-5): 6 x BertLayer(\n",
              "            (attention): BertAttention(\n",
              "              (self): BertSdpaSelfAttention(\n",
              "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (dropout): Dropout(p=0.02, inplace=False)\n",
              "              )\n",
              "              (output): BertSelfOutput(\n",
              "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "                (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "                (dropout): Dropout(p=0.02, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (intermediate): BertIntermediate(\n",
              "              (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "              (intermediate_act_fn): ReLU()\n",
              "            )\n",
              "            (output): BertOutput(\n",
              "              (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.02, inplace=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (cls): BertOnlyMLMHead(\n",
              "      (predictions): BertLMPredictionHead(\n",
              "        (transform): BertPredictionHeadTransform(\n",
              "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (transform_act_fn): ReLU()\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (decoder): Linear(in_features=256, out_features=25426, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )),\n",
              " ('bert',\n",
              "  BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(25426, 256, padding_idx=0)\n",
              "      (position_embeddings): Embedding(2048, 256)\n",
              "      (token_type_embeddings): Embedding(2, 256)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-5): 6 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (dropout): Dropout(p=0.02, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.02, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "            (intermediate_act_fn): ReLU()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.02, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )),\n",
              " ('bert.embeddings',\n",
              "  BertEmbeddings(\n",
              "    (word_embeddings): Embedding(25426, 256, padding_idx=0)\n",
              "    (position_embeddings): Embedding(2048, 256)\n",
              "    (token_type_embeddings): Embedding(2, 256)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.embeddings.word_embeddings', Embedding(25426, 256, padding_idx=0)),\n",
              " ('bert.embeddings.position_embeddings', Embedding(2048, 256)),\n",
              " ('bert.embeddings.token_type_embeddings', Embedding(2, 256)),\n",
              " ('bert.embeddings.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.embeddings.dropout', Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder',\n",
              "  BertEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x BertLayer(\n",
              "        (attention): BertAttention(\n",
              "          (self): BertSdpaSelfAttention(\n",
              "            (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (dropout): Dropout(p=0.02, inplace=False)\n",
              "          )\n",
              "          (output): BertSelfOutput(\n",
              "            (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.02, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): BertIntermediate(\n",
              "          (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "          (intermediate_act_fn): ReLU()\n",
              "        )\n",
              "        (output): BertOutput(\n",
              "          (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.02, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer',\n",
              "  ModuleList(\n",
              "    (0-5): 6 x BertLayer(\n",
              "      (attention): BertAttention(\n",
              "        (self): BertSdpaSelfAttention(\n",
              "          (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.02, inplace=False)\n",
              "        )\n",
              "        (output): BertSelfOutput(\n",
              "          (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.02, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (intermediate): BertIntermediate(\n",
              "        (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "        (intermediate_act_fn): ReLU()\n",
              "      )\n",
              "      (output): BertOutput(\n",
              "        (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.0',\n",
              "  BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSdpaSelfAttention(\n",
              "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "      (intermediate_act_fn): ReLU()\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.0.attention',\n",
              "  BertAttention(\n",
              "    (self): BertSdpaSelfAttention(\n",
              "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "    (output): BertSelfOutput(\n",
              "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.0.attention.self',\n",
              "  BertSdpaSelfAttention(\n",
              "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.0.attention.self.query',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.0.attention.self.key',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.0.attention.self.value',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.0.attention.self.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.0.attention.output',\n",
              "  BertSelfOutput(\n",
              "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.0.attention.output.dense',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.0.attention.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.0.attention.output.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.0.intermediate',\n",
              "  BertIntermediate(\n",
              "    (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (intermediate_act_fn): ReLU()\n",
              "  )),\n",
              " ('bert.encoder.layer.0.intermediate.dense',\n",
              "  Linear(in_features=256, out_features=512, bias=True)),\n",
              " ('bert.encoder.layer.0.intermediate.intermediate_act_fn', ReLU()),\n",
              " ('bert.encoder.layer.0.output',\n",
              "  BertOutput(\n",
              "    (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.0.output.dense',\n",
              "  Linear(in_features=512, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.0.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.0.output.dropout', Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.1',\n",
              "  BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSdpaSelfAttention(\n",
              "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "      (intermediate_act_fn): ReLU()\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.1.attention',\n",
              "  BertAttention(\n",
              "    (self): BertSdpaSelfAttention(\n",
              "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "    (output): BertSelfOutput(\n",
              "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.1.attention.self',\n",
              "  BertSdpaSelfAttention(\n",
              "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.1.attention.self.query',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.1.attention.self.key',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.1.attention.self.value',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.1.attention.self.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.1.attention.output',\n",
              "  BertSelfOutput(\n",
              "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.1.attention.output.dense',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.1.attention.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.1.attention.output.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.1.intermediate',\n",
              "  BertIntermediate(\n",
              "    (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (intermediate_act_fn): ReLU()\n",
              "  )),\n",
              " ('bert.encoder.layer.1.intermediate.dense',\n",
              "  Linear(in_features=256, out_features=512, bias=True)),\n",
              " ('bert.encoder.layer.1.intermediate.intermediate_act_fn', ReLU()),\n",
              " ('bert.encoder.layer.1.output',\n",
              "  BertOutput(\n",
              "    (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.1.output.dense',\n",
              "  Linear(in_features=512, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.1.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.1.output.dropout', Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.2',\n",
              "  BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSdpaSelfAttention(\n",
              "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "      (intermediate_act_fn): ReLU()\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.2.attention',\n",
              "  BertAttention(\n",
              "    (self): BertSdpaSelfAttention(\n",
              "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "    (output): BertSelfOutput(\n",
              "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.2.attention.self',\n",
              "  BertSdpaSelfAttention(\n",
              "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.2.attention.self.query',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.2.attention.self.key',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.2.attention.self.value',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.2.attention.self.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.2.attention.output',\n",
              "  BertSelfOutput(\n",
              "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.2.attention.output.dense',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.2.attention.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.2.attention.output.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.2.intermediate',\n",
              "  BertIntermediate(\n",
              "    (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (intermediate_act_fn): ReLU()\n",
              "  )),\n",
              " ('bert.encoder.layer.2.intermediate.dense',\n",
              "  Linear(in_features=256, out_features=512, bias=True)),\n",
              " ('bert.encoder.layer.2.intermediate.intermediate_act_fn', ReLU()),\n",
              " ('bert.encoder.layer.2.output',\n",
              "  BertOutput(\n",
              "    (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.2.output.dense',\n",
              "  Linear(in_features=512, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.2.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.2.output.dropout', Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.3',\n",
              "  BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSdpaSelfAttention(\n",
              "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "      (intermediate_act_fn): ReLU()\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.3.attention',\n",
              "  BertAttention(\n",
              "    (self): BertSdpaSelfAttention(\n",
              "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "    (output): BertSelfOutput(\n",
              "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.3.attention.self',\n",
              "  BertSdpaSelfAttention(\n",
              "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.3.attention.self.query',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.3.attention.self.key',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.3.attention.self.value',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.3.attention.self.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.3.attention.output',\n",
              "  BertSelfOutput(\n",
              "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.3.attention.output.dense',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.3.attention.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.3.attention.output.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.3.intermediate',\n",
              "  BertIntermediate(\n",
              "    (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (intermediate_act_fn): ReLU()\n",
              "  )),\n",
              " ('bert.encoder.layer.3.intermediate.dense',\n",
              "  Linear(in_features=256, out_features=512, bias=True)),\n",
              " ('bert.encoder.layer.3.intermediate.intermediate_act_fn', ReLU()),\n",
              " ('bert.encoder.layer.3.output',\n",
              "  BertOutput(\n",
              "    (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.3.output.dense',\n",
              "  Linear(in_features=512, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.3.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.3.output.dropout', Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.4',\n",
              "  BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSdpaSelfAttention(\n",
              "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "      (intermediate_act_fn): ReLU()\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.4.attention',\n",
              "  BertAttention(\n",
              "    (self): BertSdpaSelfAttention(\n",
              "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "    (output): BertSelfOutput(\n",
              "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.4.attention.self',\n",
              "  BertSdpaSelfAttention(\n",
              "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.4.attention.self.query',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.4.attention.self.key',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.4.attention.self.value',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.4.attention.self.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.4.attention.output',\n",
              "  BertSelfOutput(\n",
              "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.4.attention.output.dense',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.4.attention.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.4.attention.output.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.4.intermediate',\n",
              "  BertIntermediate(\n",
              "    (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (intermediate_act_fn): ReLU()\n",
              "  )),\n",
              " ('bert.encoder.layer.4.intermediate.dense',\n",
              "  Linear(in_features=256, out_features=512, bias=True)),\n",
              " ('bert.encoder.layer.4.intermediate.intermediate_act_fn', ReLU()),\n",
              " ('bert.encoder.layer.4.output',\n",
              "  BertOutput(\n",
              "    (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.4.output.dense',\n",
              "  Linear(in_features=512, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.4.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.4.output.dropout', Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.5',\n",
              "  BertLayer(\n",
              "    (attention): BertAttention(\n",
              "      (self): BertSdpaSelfAttention(\n",
              "        (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "      (output): BertSelfOutput(\n",
              "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "        (dropout): Dropout(p=0.02, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (intermediate): BertIntermediate(\n",
              "      (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "      (intermediate_act_fn): ReLU()\n",
              "    )\n",
              "    (output): BertOutput(\n",
              "      (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.5.attention',\n",
              "  BertAttention(\n",
              "    (self): BertSdpaSelfAttention(\n",
              "      (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "    (output): BertSelfOutput(\n",
              "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.02, inplace=False)\n",
              "    )\n",
              "  )),\n",
              " ('bert.encoder.layer.5.attention.self',\n",
              "  BertSdpaSelfAttention(\n",
              "    (query): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (key): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (value): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.5.attention.self.query',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.5.attention.self.key',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.5.attention.self.value',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.5.attention.self.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.5.attention.output',\n",
              "  BertSelfOutput(\n",
              "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.5.attention.output.dense',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.5.attention.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.5.attention.output.dropout',\n",
              "  Dropout(p=0.02, inplace=False)),\n",
              " ('bert.encoder.layer.5.intermediate',\n",
              "  BertIntermediate(\n",
              "    (dense): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (intermediate_act_fn): ReLU()\n",
              "  )),\n",
              " ('bert.encoder.layer.5.intermediate.dense',\n",
              "  Linear(in_features=256, out_features=512, bias=True)),\n",
              " ('bert.encoder.layer.5.intermediate.intermediate_act_fn', ReLU()),\n",
              " ('bert.encoder.layer.5.output',\n",
              "  BertOutput(\n",
              "    (dense): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.02, inplace=False)\n",
              "  )),\n",
              " ('bert.encoder.layer.5.output.dense',\n",
              "  Linear(in_features=512, out_features=256, bias=True)),\n",
              " ('bert.encoder.layer.5.output.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('bert.encoder.layer.5.output.dropout', Dropout(p=0.02, inplace=False)),\n",
              " ('cls',\n",
              "  BertOnlyMLMHead(\n",
              "    (predictions): BertLMPredictionHead(\n",
              "      (transform): BertPredictionHeadTransform(\n",
              "        (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "        (transform_act_fn): ReLU()\n",
              "        (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "      (decoder): Linear(in_features=256, out_features=25426, bias=True)\n",
              "    )\n",
              "  )),\n",
              " ('cls.predictions',\n",
              "  BertLMPredictionHead(\n",
              "    (transform): BertPredictionHeadTransform(\n",
              "      (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (transform_act_fn): ReLU()\n",
              "      (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): Linear(in_features=256, out_features=25426, bias=True)\n",
              "  )),\n",
              " ('cls.predictions.transform',\n",
              "  BertPredictionHeadTransform(\n",
              "    (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (transform_act_fn): ReLU()\n",
              "    (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n",
              "  )),\n",
              " ('cls.predictions.transform.dense',\n",
              "  Linear(in_features=256, out_features=256, bias=True)),\n",
              " ('cls.predictions.transform.transform_act_fn', ReLU()),\n",
              " ('cls.predictions.transform.LayerNorm',\n",
              "  LayerNorm((256,), eps=1e-12, elementwise_affine=True)),\n",
              " ('cls.predictions.decoder',\n",
              "  Linear(in_features=256, out_features=25426, bias=True))]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.model.named_modules())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a838cebc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing 2D input (batch, seq):\n",
            "  Shape: torch.Size([4, 128])\n",
            "  Output shape: torch.Size([4, 128, 256])\n",
            "  SUCCESS\n",
            "\n",
            "Testing 3D input (n_adapt, batch, seq):\n",
            "  Shape: torch.Size([3, 4, 128])\n",
            "  FAILED: too many values to unpack (expected 2)\n",
            "\n",
            "Testing flattened 3D -> 2D:\n",
            "  Flattened shape: torch.Size([12, 128])\n",
            "  Output shape: torch.Size([12, 128, 256])\n",
            "  SUCCESS\n"
          ]
        }
      ],
      "source": [
        "# Test if BERT can handle extra batch dimensions\n",
        "# Standard: input_ids (batch, seq) \n",
        "# Question: can we pass (n_adapt, batch, seq) without flattening?\n",
        "\n",
        "import torch\n",
        "\n",
        "# Get the underlying BERT model\n",
        "bert = model.model.bert\n",
        "\n",
        "# Create test inputs\n",
        "batch_size = 4\n",
        "seq_len = 128\n",
        "n_adapt = 3\n",
        "\n",
        "# Standard 2D input\n",
        "input_ids_2d = torch.randint(1, 1000, (batch_size, seq_len))\n",
        "\n",
        "# 3D input with extra adapter dimension  \n",
        "input_ids_3d = torch.randint(1, 1000, (n_adapt, batch_size, seq_len))\n",
        "\n",
        "print(\"Testing 2D input (batch, seq):\")\n",
        "print(f\"  Shape: {input_ids_2d.shape}\")\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        out_2d = bert(input_ids_2d)\n",
        "    print(f\"  Output shape: {out_2d.last_hidden_state.shape}\")\n",
        "    print(\"  SUCCESS\")\n",
        "except Exception as e:\n",
        "    print(f\"  FAILED: {e}\")\n",
        "\n",
        "print(\"\\nTesting 3D input (n_adapt, batch, seq):\")\n",
        "print(f\"  Shape: {input_ids_3d.shape}\")\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        out_3d = bert(input_ids_3d)\n",
        "    print(f\"  Output shape: {out_3d.last_hidden_state.shape}\")\n",
        "    print(\"  SUCCESS - BERT handles extra batch dims natively!\")\n",
        "except Exception as e:\n",
        "    print(f\"  FAILED: {e}\")\n",
        "\n",
        "print(\"\\nTesting flattened 3D -> 2D:\")\n",
        "input_ids_flat = input_ids_3d.reshape(n_adapt * batch_size, seq_len)\n",
        "print(f\"  Flattened shape: {input_ids_flat.shape}\")\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        out_flat = bert(input_ids_flat)\n",
        "    print(f\"  Output shape: {out_flat.last_hidden_state.shape}\")\n",
        "    print(\"  SUCCESS\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"  FAILED: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8944e808",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
